{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6ceb41",
   "metadata": {},
   "source": [
    "3-Class Grouping Strategy for Heart Disease Severity\n",
    "=====================================================\n",
    "\n",
    "Grouping Strategy:\n",
    "- Class 0: No Disease (original class 0)\n",
    "- Class 1: Mild to Moderate (original classes 1-2)\n",
    "- Class 2: Severe to Very Severe (original classes 3-4)\n",
    "\n",
    "Rationale:\n",
    "1. Addresses extreme class imbalance (15:1 ratio in 5-class)\n",
    "2. Creates clinically meaningful groups\n",
    "3. Improves F1-score by reducing class confusion\n",
    "4. Maintains severity ordering for ordinal classification\n",
    "\n",
    "Expected Improvement:\n",
    "- 5-class F1: 0.5863 → 3-class F1: target 0.75+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa52b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"3-CLASS GROUPING EXPERIMENT\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGrouping Strategy:\")\n",
    "print(\"  Class 0: No Disease (original 0)\")\n",
    "print(\"  Class 1: Mild-Moderate (original 1-2)\")\n",
    "print(\"  Class 2: Severe-Critical (original 3-4)\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30a49e",
   "metadata": {},
   "source": [
    "## 1. LOAD AND PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29287bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[1/6] Loading data...\")\n",
    "\n",
    "# Load the original dataset\n",
    "data_path = Path(\"../data/raw/heart_disease_uci.csv\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Drop non-feature columns\n",
    "columns_to_drop = ['id', 'dataset']\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "\n",
    "print(f\"✓ Loaded {len(df)} samples\")\n",
    "print(f\"✓ Dataset columns: {list(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd230",
   "metadata": {},
   "source": [
    "## 2. APPLY 3-CLASS GROUPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[2/6] Applying 3-class grouping...\")\n",
    "\n",
    "# Original class distribution\n",
    "original_target = 'num' if 'num' in df.columns else 'target'\n",
    "print(f\"\\nOriginal 5-class distribution:\")\n",
    "print(df[original_target].value_counts().sort_index())\n",
    "\n",
    "# Create 3-class grouping\n",
    "def group_severity(severity):\n",
    "    \"\"\"\n",
    "    Group 5 severity levels into 3 clinical categories\n",
    "\n",
    "    0: No Disease (healthy)\n",
    "    1: Mild-Moderate (needs monitoring/lifestyle changes)\n",
    "    2: Severe-Critical (needs urgent medical intervention)\n",
    "    \"\"\"\n",
    "    if severity == 0:\n",
    "        return 0  # No disease\n",
    "    elif severity in [1, 2]:\n",
    "        return 1  # Mild-Moderate\n",
    "    else:  # severity in [3, 4]\n",
    "        return 2  # Severe-Critical\n",
    "\n",
    "df['target_3class'] = df[original_target].apply(group_severity)\n",
    "\n",
    "print(f\"\\nNew 3-class distribution:\")\n",
    "class_counts = df['target_3class'].value_counts().sort_index()\n",
    "print(class_counts)\n",
    "print(f\"\\nImbalance ratio: {class_counts.max() / class_counts.min():.2f}:1\")\n",
    "print(f\"Improvement: 15:1 → {class_counts.max() / class_counts.min():.2f}:1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296e2ec",
   "metadata": {},
   "source": [
    "## 3. FEATURE ENGINEERING (SAME AS BEFORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[3/6] Feature engineering...\")\n",
    "\n",
    "# Create age groups (WHO categories)\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 40, 60, 80, 100],\n",
    "                          labels=[0, 1, 2, 3])  # Young, Middle, Senior, Elderly\n",
    "\n",
    "# Blood pressure categories (AHA guidelines)\n",
    "df['bp_category'] = pd.cut(df['trestbps'], bins=[0, 120, 130, 140, 200],\n",
    "                            labels=[0, 1, 2, 3])  # Normal, Elevated, Stage1, Stage2\n",
    "\n",
    "# Cholesterol categories\n",
    "df['chol_category'] = pd.cut(df['chol'], bins=[0, 200, 240, 600],\n",
    "                              labels=[0, 1, 2])  # Desirable, Borderline, High\n",
    "\n",
    "# Heart rate reserve\n",
    "df['hr_reserve'] = 220 - df['age'] - df['thalch']\n",
    "\n",
    "# Composite cardiovascular risk score\n",
    "df['cv_risk_score'] = (\n",
    "    (df['age'] > 55).astype(int) * 2 +\n",
    "    (df['trestbps'] > 140).astype(int) * 2 +\n",
    "    (df['chol'] > 240).astype(int) +\n",
    "    (df['fbs'] == 1).astype(int) +\n",
    "    (df['exang'] == 1).astype(int) * 2 +\n",
    "    (df['oldpeak'] > 2).astype(int) * 3\n",
    ")\n",
    "\n",
    "print(f\"✓ Created 5 engineered features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbad3f",
   "metadata": {},
   "source": [
    "## 4. PREPROCESSING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9e5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[4/6] Preprocessing pipeline...\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop([original_target, 'target_3class'], axis=1)\n",
    "y = df['target_3class']\n",
    "\n",
    "# Train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"✓ Train set: {len(X_train)} samples\")\n",
    "print(f\"✓ Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal',\n",
    "                    'age_group', 'bp_category', 'chol_category']\n",
    "\n",
    "# Label encoding for categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in X_train.columns:\n",
    "        le = LabelEncoder()\n",
    "        X_train[col] = le.fit_transform(X_train[col].astype(str))\n",
    "        X_test[col] = le.transform(X_test[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "print(f\"✓ Encoded {len(label_encoders)} categorical features\")\n",
    "\n",
    "# KNN Imputation for missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_test_imputed = pd.DataFrame(\n",
    "    imputer.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"✓ Imputed missing values\")\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "print(f\"✓ Scaled features\")\n",
    "\n",
    "# BorderlineSMOTE for class balance\n",
    "print(\"\\nClass distribution before SMOTE:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "\n",
    "smote = BorderlineSMOTE(random_state=RANDOM_STATE, k_neighbors=3, kind='borderline-1')\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2b1ef1",
   "metadata": {},
   "source": [
    "## 5. TRAIN 3-CLASS MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec833814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[5/6] Training models...\")\n",
    "\n",
    "models = {}\n",
    "results = []\n",
    "\n",
    "# Model 1: XGBoost Baseline\n",
    "print(\"\\n--- XGBoost Baseline ---\")\n",
    "xgb_baseline = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    min_child_weight=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "xgb_baseline.fit(X_train_smote, y_train_smote)\n",
    "y_pred_baseline = xgb_baseline.predict(X_test_scaled)\n",
    "\n",
    "f1_baseline = f1_score(y_test, y_pred_baseline, average='weighted')\n",
    "acc_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"Test F1-Score: {f1_baseline:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_baseline:.4f}\")\n",
    "\n",
    "models['xgb_baseline'] = xgb_baseline\n",
    "results.append({\n",
    "    'Model': 'XGBoost Baseline',\n",
    "    'Test F1': f1_baseline,\n",
    "    'Test Accuracy': acc_baseline\n",
    "})\n",
    "\n",
    "# Model 2: XGBoost with Ordinal Weights\n",
    "print(\"\\n--- XGBoost Ordinal Weights ---\")\n",
    "\n",
    "# Create ordinal sample weights for 3 classes\n",
    "# Class 0: weight 1.0 (no disease)\n",
    "# Class 1: weight 1.5 (mild-moderate)\n",
    "# Class 2: weight 2.0 (severe-critical)\n",
    "sample_weights = np.array([1.0 + 0.5 * y for y in y_train_smote])\n",
    "\n",
    "xgb_ordinal = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=7,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    min_child_weight=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "xgb_ordinal.fit(X_train_smote, y_train_smote, sample_weight=sample_weights)\n",
    "y_pred_ordinal = xgb_ordinal.predict(X_test_scaled)\n",
    "\n",
    "f1_ordinal = f1_score(y_test, y_pred_ordinal, average='weighted')\n",
    "acc_ordinal = accuracy_score(y_test, y_pred_ordinal)\n",
    "\n",
    "print(f\"Test F1-Score: {f1_ordinal:.4f}\")\n",
    "print(f\"Test Accuracy: {acc_ordinal:.4f}\")\n",
    "\n",
    "models['xgb_ordinal'] = xgb_ordinal\n",
    "results.append({\n",
    "    'Model': 'XGBoost Ordinal Weights',\n",
    "    'Test F1': f1_ordinal,\n",
    "    'Test Accuracy': acc_ordinal\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6125ad56",
   "metadata": {},
   "source": [
    "## 6. DETAILED EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87bb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[6/6] Detailed evaluation...\")\n",
    "\n",
    "# Select best model\n",
    "best_model_name = 'xgb_ordinal' if f1_ordinal > f1_baseline else 'xgb_baseline'\n",
    "best_model = models[best_model_name]\n",
    "y_pred_best = xgb_ordinal.predict(X_test_scaled) if f1_ordinal > f1_baseline else y_pred_baseline\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"BEST MODEL: {best_model_name.upper()}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['No Disease', 'Mild-Moderate', 'Severe-Critical']\n",
    "print(classification_report(y_test, y_pred_best, target_names=class_names, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"\\n            Predicted\")\n",
    "print(\"              0     1     2\")\n",
    "print(f\"Actual  0  {cm[0][0]:4d}  {cm[0][1]:4d}  {cm[0][2]:4d}\")\n",
    "print(f\"        1  {cm[1][0]:4d}  {cm[1][1]:4d}  {cm[1][2]:4d}\")\n",
    "print(f\"        2  {cm[2][0]:4d}  {cm[2][1]:4d}  {cm[2][2]:4d}\")\n",
    "\n",
    "# Per-class metrics\n",
    "print(\"\\nPer-class F1-scores:\")\n",
    "f1_per_class = f1_score(y_test, y_pred_best, average=None)\n",
    "for i, (class_name, f1) in enumerate(zip(class_names, f1_per_class)):\n",
    "    print(f\"  {class_name:20s}: {f1:.4f}\")\n",
    "\n",
    "# Comparison with 5-class\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: 5-CLASS vs 3-CLASS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"5-class F1-score: 0.5863\")\n",
    "print(f\"3-class F1-score: {f1_score(y_test, y_pred_best, average='weighted'):.4f}\")\n",
    "improvement = (f1_score(y_test, y_pred_best, average='weighted') - 0.5863) / 0.5863 * 100\n",
    "print(f\"Improvement: {improvement:+.2f}%\")\n",
    "\n",
    "if f1_score(y_test, y_pred_best, average='weighted') >= 0.75:\n",
    "    print(\"\\n✅ TARGET ACHIEVED: F1 ≥ 0.75\")\n",
    "else:\n",
    "    gap = 0.75 - f1_score(y_test, y_pred_best, average='weighted')\n",
    "    print(f\"\\n⚠️  Gap to target (0.75): -{gap:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefa2e8",
   "metadata": {},
   "source": [
    "## 7. SAVE MODELS AND ARTIFACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b6dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODELS AND ARTIFACTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save best 3-class model\n",
    "model_path = models_dir / \"best_3class_model.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "print(f\"✓ Saved model: {model_path}\")\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "preprocessing_artifacts = {\n",
    "    'label_encoders': label_encoders,\n",
    "    'imputer': imputer,\n",
    "    'scaler': scaler,\n",
    "    'smote': smote,\n",
    "    'feature_names': list(X.columns)\n",
    "}\n",
    "\n",
    "artifacts_path = models_dir / \"preprocessing_artifacts_3class.pkl\"\n",
    "with open(artifacts_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_artifacts, f)\n",
    "print(f\"✓ Saved preprocessing artifacts: {artifacts_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_name': 'XGBoost 3-Class Severity',\n",
    "    'model_type': best_model_name,\n",
    "    'num_classes': 3,\n",
    "    'class_names': class_names,\n",
    "    'class_mapping': {\n",
    "        0: 'No Disease',\n",
    "        1: 'Mild-Moderate',\n",
    "        2: 'Severe-Critical'\n",
    "    },\n",
    "    'original_to_grouped': {\n",
    "        0: 0,  # No disease → No disease\n",
    "        1: 1,  # Mild → Mild-Moderate\n",
    "        2: 1,  # Moderate → Mild-Moderate\n",
    "        3: 2,  # Severe → Severe-Critical\n",
    "        4: 2   # Very Severe → Severe-Critical\n",
    "    },\n",
    "    'performance': {\n",
    "        'test_f1_weighted': float(f1_score(y_test, y_pred_best, average='weighted')),\n",
    "        'test_f1_macro': float(f1_score(y_test, y_pred_best, average='macro')),\n",
    "        'test_accuracy': float(accuracy_score(y_test, y_pred_best)),\n",
    "        'test_precision': float(precision_score(y_test, y_pred_best, average='weighted')),\n",
    "        'test_recall': float(recall_score(y_test, y_pred_best, average='weighted')),\n",
    "        'f1_per_class': [float(x) for x in f1_per_class]\n",
    "    },\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "metadata_path = models_dir / \"model_metadata_3class.pkl\"\n",
    "with open(metadata_path, 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "print(f\"✓ Saved metadata: {metadata_path}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_path = Path(\"../results/three_class_grouping_results.csv\")\n",
    "results_path.parent.mkdir(exist_ok=True)\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"✓ Saved results: {results_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nBest 3-Class Model: {best_model_name}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_best, average='weighted'):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Update Flask API to use best_3class_model.pkl\")\n",
    "print(\"2. Update frontend to display 3 severity levels\")\n",
    "print(\"3. Update documentation with new grouping strategy\")\n",
    "print(\"=\"*70)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
